{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rondi91/BISINDO/blob/main/TEST_GBI_WITH_ALL_DATASET_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GlkGq-wG_XM"
      },
      "source": [
        "# **2.Instalation TFOD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kMlXyHFH2_P"
      },
      "source": [
        "## **2.1 Install Object Detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOQTtIgac4Se"
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models git repository\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/tensorflow/models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P297VQSGdhq3"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow Object Detection API\n",
        "\n",
        "%cd models/research\n",
        "\n",
        "# Compile protos.\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# Install TensorFlow Object Detection API.\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install --use-feature=in-tree-build ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byU6pGafZr3A"
      },
      "outputs": [],
      "source": [
        "!cp {ANNOTATION_PATH+'/arial.ttf'} /usr/share/fonts/truetype\n",
        "!cp {ANNOTATION_PATH+'/arial.ttf'} /usr/local/lib/python3.7/dist-packages/object_detection/utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XRFTH9BIBVT"
      },
      "source": [
        "## **2.2 Test the model builder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpasmWV7dpRf"
      },
      "outputs": [],
      "source": [
        "#Test Instalation objec detection\n",
        "!python object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcVsujyHozqE"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r59uBw2jHKK2"
      },
      "source": [
        "### **finish instalation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUT_L6bjOu_B"
      },
      "source": [
        "# **1.PREPARING THE WORKSPACE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miqMSoFhziVS"
      },
      "source": [
        "## 1.1 Mount Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Si3sOwi7e06"
      },
      "outputs": [],
      "source": [
        "# 1.1 Mount Google Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG-5g4RvdQ0H"
      },
      "outputs": [],
      "source": [
        "# Tensorflow version\n",
        "import tensorflow as tf \n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL2bbe8nhDc6"
      },
      "source": [
        "#  **3.Preprocessing Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv0pIFhUKFjW"
      },
      "source": [
        "##  3.1 Clone Dataset Image "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDYCkXj8x1_F"
      },
      "outputs": [],
      "source": [
        "# #Clone dataset image \n",
        "\n",
        "# #cd into tensorflow/workspace\n",
        "# %cd {BASE_PATH}/workspace/\n",
        "\n",
        "# !git clone https://github.com/rondi91/images.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcb3SAHB6Ntc"
      },
      "source": [
        "## **3.4 CEK DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yznxVK0j6NDX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(ANNOTATION_PATH+\"/ALL_DATASET.csv\")\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJdMupjk7iiL"
      },
      "outputs": [],
      "source": [
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b-4m6Wl8HUt"
      },
      "outputs": [],
      "source": [
        "jml_class=data['class'].drop_duplicates().count()\n",
        "# class_name=data['class'].drop_duplicates()\n",
        "class_name=data.groupby('class')['class'].count()\n",
        "\n",
        "print('======class name ============')\n",
        "print(class_name)\n",
        "print('======class total =====')\n",
        "print(jml_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suaq5Be0D2p6"
      },
      "outputs": [],
      "source": [
        "print('dimensi train.csv ')\n",
        "train = pd.read_csv(ANNOTATION_PATH+\"/train.csv\")\n",
        "train.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcQIRpQ0ELih"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(ANNOTATION_PATH+\"/test.csv\")\n",
        "test.shape "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Isizxnjz7GH"
      },
      "source": [
        "# 1.2 Prepare & Clone Template TFOD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDkIsATInS0C"
      },
      "outputs": [],
      "source": [
        "# buat folder penyimpanan deteksi BISINDO\n",
        "BISINDO_FOLDER = '25.BISINDO_NAMA_HARI'\n",
        "!mkdir {'/content/gdrive/MyDrive/'+ BISINDO_FOLDER}\n",
        "# buat base_url root\n",
        "BASE_PATH = '/content/gdrive/MyDrive/'+BISINDO_FOLDER+'/Tensorflow'\n",
        "\n",
        "# Clone template Train TFOD dari github\n",
        "%cd /content/gdrive/MyDrive/{BISINDO_FOLDER}\n",
        "!git clone https://github.com/rondi91/Tensorflow.git\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **1.3 Set Up Variable Path**"
      ],
      "metadata": {
        "id": "JnimmlyE_nwA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1crrus6pETIX"
      },
      "outputs": [],
      "source": [
        "# Set Up Variable Path\n",
        "\n",
        "NUM_STEPS = 10000\n",
        "LR_TOTAL_STEPS =10000\n",
        "BATCH_SIZE=64\n",
        "CUSTOM_MODEL_NAME = 'my_ssd_mobnet_5k_cek_time'+str(BATCH_SIZE)          #folder Train project\n",
        "OUTPUT_MODEL= 'my_ssd_mobnet_5k_cek_time'+ str(NUM_STEPS)+'_'+ str(BATCH_SIZE)                    #OUTPUT MODEL TRAIN\n",
        "\n",
        "\n",
        "WORKSPACE_PATH = BASE_PATH+ '/workspace'\n",
        "SCRIPTS_PATH =  WORKSPACE_PATH+'/scripts'\n",
        "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
        "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
        "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
        "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n",
        "EXPORTED_MODELS = WORKSPACE_PATH+'/exported-models'\n",
        "\n",
        "# my_ssd_mobnet5_100batch_5k\n",
        "\n",
        "#APIMODEL_PATH = WORKSPACE_PATH+ 'Tensorflow/models'\n",
        "#CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n",
        "#CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_resnet101_v1_fpn/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngritkBjyETx"
      },
      "source": [
        "# **5.MODEL EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JISAAfK-inS"
      },
      "outputs": [],
      "source": [
        "#start the Tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir={MODEL_PATH +'/'+CUSTOM_MODEL_NAME}\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir=models/[name_of_pre-trained-model_you_downloaded]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqt8HidxzK2J"
      },
      "source": [
        "#  **7.Testing  Model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQWLOON4dZt"
      },
      "source": [
        "We have finished training and exporting our model. It's time to test our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz2jiOgJ4mUK"
      },
      "outputs": [],
      "source": [
        "# 5.1 Loading the saved_model\n",
        "\n",
        "#Loading the saved_model\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "PATH_TO_SAVED_MODEL= EXPORTED_MODELS+'/'+OUTPUT_MODEL+'/saved_model'\n",
        "\n",
        "print('Loading model...', end='')\n",
        "\n",
        "# Load saved model and build the detection function\n",
        "detect_fn=tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CEK MODEL\n",
        "PATH_TO_SAVED_MODEL"
      ],
      "metadata": {
        "id": "VNLbVl2dnTsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX--5tHy4-S1"
      },
      "outputs": [],
      "source": [
        "# 5.2 Testing the Model.\n",
        "\n",
        "#Loading the label_map\n",
        "category_index=label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+\"/label_map.pbtxt\",use_display_name=True)\n",
        "\n",
        "#category_index=label_map_util.create_category_index_from_labelmap([path_to_label_map],use_display_name=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UURD_H_c5grh"
      },
      "outputs": [],
      "source": [
        "#Loading the image\n",
        "img=[\n",
        "      IMAGE_PATH+'/image_testing_rondi/SENIN/image_RONDI_215.jpg',\n",
        "      IMAGE_PATH+'/image_testing_rondi/SELASA/image_RONDI_269.jpg',\n",
        "      IMAGE_PATH+'/image_testing_rondi/RABU/image_RONDI_233.jpg',\n",
        "      IMAGE_PATH+'/image_testing_rondi/KAMIS/image_RONDI_2192.jpg',\n",
        "      IMAGE_PATH+'/image_testing_rondi/JUMAT/image_RONDI_2260.jpg',\n",
        "      IMAGE_PATH+'/image_testing_rondi/SABTU/image_RONDI_2321.jpg',\n",
        "      IMAGE_PATH+'/image_testing_rondi/MINGGU/image_RONDI_2356.jpg',\n",
        "     ]\n",
        "print(img)\n",
        "\n",
        "#list containing paths of all the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swO5JcDurLne"
      },
      "source": [
        "## 5.1.Show image testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR5KWQ7qrAhu"
      },
      "outputs": [],
      "source": [
        "#Step 16- show image testing\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "for image_path in img:\n",
        "\n",
        "    print('Running inference for {}... '.format(image_path), end='')\n",
        "    image_np=load_image_into_numpy_array(image_path)\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    %matplotlib inline\n",
        "    plt.figure()\n",
        "    plt.imshow(image_np)\n",
        "    print('Done')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKA0QJBGrUdG"
      },
      "source": [
        "## 5.2 Show Deteksi on image "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir {IMAGE_PATH}/output_deteksi_image/dir_BATH_IMG_DETECT"
      ],
      "metadata": {
        "id": "c18Vnfe6ua3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# membuat folder penyimpanan gambar hasil deteksi\n",
        "dir_img_detection =str(NUM_STEPS)\n",
        "!mkdir {IMAGE_PATH+'/output_deteksi_image/'+ CUSTOM_MODEL_NAME}\n",
        "!mkdir {IMAGE_PATH+'/output_deteksi_image/'+ CUSTOM_MODEL_NAME+'/' +dir_img_detection}\n"
      ],
      "metadata": {
        "id": "1pDqfqZnQNj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DETEC_SAVE_IMG"
      ],
      "metadata": {
        "id": "SAAVf2Syfk7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 17- Running the Inference.\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "for image_path in img:\n",
        "\n",
        "    print('Running inference for {}... '.format(image_path), end='')\n",
        "    image_np=load_image_into_numpy_array(image_path)\n",
        "\n",
        "    # Things to try:\n",
        "    # Flip horizontally\n",
        "    # image_np = np.fliplr(image_np).copy()\n",
        "    # Convert image to grayscale\n",
        "    # image_np = np.tile(\n",
        "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "    input_tensor=tf.convert_to_tensor(image_np)\n",
        "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "    input_tensor=input_tensor[tf.newaxis, ...]\n",
        "\n",
        "    # input_tensor = np.expand_dims(image_np, 0)\n",
        "    detections=detect_fn(input_tensor)\n",
        "\n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections=int(detections.pop('num_detections'))\n",
        "    detections={key:value[0,:num_detections].numpy()\n",
        "                   for key,value in detections.items()}\n",
        "    detections['num_detections']=num_detections\n",
        "\n",
        "    # detection_classes should be ints.\n",
        "    detections['detection_classes']=detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "    image_np_with_detections=image_np.copy()\n",
        "       \n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "          image_np_with_detections,\n",
        "          detections['detection_boxes'],\n",
        "          detections['detection_classes'],\n",
        "          detections['detection_scores'],\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          max_boxes_to_draw=1,     #max number of bounding boxes in the image\n",
        "          min_score_thresh=.3,      #min prediction threshold\n",
        "          agnostic_mode=False,\n",
        "          line_thickness=3,\n",
        "          keypoint_edges=4,\n",
        "          )\n",
        "    \n",
        "    %matplotlib inline\n",
        "    im_rgb = cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB)\n",
        "    name= os.path.basename(image_path)\n",
        "    plt.figure(figsize=[10,10],dpi=100)\n",
        "    plt.imshow(image_np_with_detections)\n",
        "    # cv2.imwrite(IMAGE_PATH+'/output_deteksi_image/'+dir_img_detection+'/'+name, im_rgb)\n",
        "    cv2.imwrite(IMAGE_PATH+'/output_deteksi_image/'+ CUSTOM_MODEL_NAME+'/' +dir_img_detection+'/'+ name, im_rgb)\n",
        "    print('Done')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bOExaMkg9QIo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "miqMSoFhziVS",
        "9GlkGq-wG_XM",
        "pDZd_WY_gLJ3",
        "l_YEdVTnYbEI",
        "swO5JcDurLne"
      ],
      "machine_shape": "hm",
      "name": "TEST_GBI_WITH ALL DATASET v2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ee6ef56facda7503055c4941e2c2083c4bcc9ecb08a66ac58f56d3b05ea5e5fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}